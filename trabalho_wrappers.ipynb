{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ee9226",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d4cf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium e wrappers\n",
    "import gymnasium as gym\n",
    "\n",
    "# Stable-Baselines3\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f69d3",
   "metadata": {},
   "source": [
    "# Wrappers para customizações do LunarLander-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22601665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Reward Shaping: penaliza ângulo, distância e dá bónus por aterragem perfeita\n",
    "\n",
    "class RewardShapingWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env, angle_penalty=20.0, dist_penalty=0.5, land_bonus=100.0):\n",
    "        super().__init__(env)\n",
    "        self.angle_penalty = angle_penalty\n",
    "        self.dist_penalty = dist_penalty\n",
    "        self.land_bonus = land_bonus\n",
    "\n",
    "\n",
    "    def reward(self, reward):\n",
    "    # Access internal state from env.unwrapped.state if available\n",
    "        try:\n",
    "            state = self.env.unwrapped.state\n",
    "            # state layout (LunarLander): [x, y, vx, vy, angle, angular_velocity, left_contact, right_contact]\n",
    "            if state[6] and state[7]:  # ambos os contactos\n",
    "                shaping += self.land_bonus\n",
    "            angle = float(state[4])\n",
    "            angular_velocity = float(state[5])\n",
    "            x = float(state[0])\n",
    "            y = float(state[1])\n",
    "            # distance to pad (x axis only): penalise being far from x=0\n",
    "            dist = abs(x)\n",
    "            shaping = - self.angle_penalty * (abs(angle) + abs(angular_velocity)) - self.dist_penalty * dist\n",
    "        except Exception:\n",
    "            shaping = 0.0\n",
    "     \n",
    "        return reward + shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4261db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindForceWrapper(gym.Wrapper):\n",
    "    \"\"\"Aplica perturbação lateral (vento) à velocidade horizontal 'observada'.\"\"\"\n",
    "    def __init__(self, env, wind_strength=0.02, deterministic=False):\n",
    "        super().__init__(env)\n",
    "        self.wind_strength = wind_strength\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def step(self, action):\n",
    "        # Mantemos a assinatura Gymnasium: (obs, reward, terminated, truncated, info)\n",
    "        out = self.env.step(action)\n",
    "        # Pode ser 5-tuple (gymnasium) ou 4-tuple dependendo do wrapper; tratamos ambos\n",
    "        if len(out) == 5:\n",
    "            obs, reward, terminated, truncated, info = out\n",
    "            done_flag = (terminated or truncated)\n",
    "        else:\n",
    "            obs, reward, done_flag, info = out\n",
    "            terminated = done_flag; truncated = False\n",
    "\n",
    "        if isinstance(obs, np.ndarray) and obs.shape[0] >= 3:\n",
    "            wind = self.wind_strength if self.deterministic else self.wind_strength * (2*np.random.rand()-1)\n",
    "            obs = obs.copy()\n",
    "            obs[2] += wind\n",
    "            reward -= abs(wind) * 5.0\n",
    "\n",
    "        # devolver no mesmo formato do original\n",
    "        if len(out) == 5:\n",
    "            return obs, reward, terminated, truncated, info\n",
    "        else:\n",
    "            return obs, reward, done_flag, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0f1c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObservationNoiseWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Add gaussian noise to observations to make the task more robust.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noise_std=0.01):\n",
    "        super().__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            return observation + np.random.normal(scale=self.noise_std, size=observation.shape)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b674",
   "metadata": {},
   "source": [
    "# Criação do ambiente customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d41948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_factory(env_id='LunarLander-v3', seed=None, config_name='orig', monitor_dir=None):\n",
    "    \"\"\"\n",
    "    Retorna uma função _init compatível com DummyVecEnv que:\n",
    "     - cria env,\n",
    "     - aplica wrappers consoante config_name,\n",
    "     - envolve com Monitor(escreve monitor.csv em monitor_dir).\n",
    "    config_name em {'orig', 'reward', 'wind', 'noise', 'all'}\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        if seed is not None:\n",
    "            # seed reset (Gymnasium)\n",
    "            try:\n",
    "                env.reset(seed=seed)\n",
    "            except TypeError:\n",
    "                env.reset()\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "\n",
    "        # Aplicar wrappers conforme config\n",
    "        if config_name == 'reward':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, land_bonus=100.0)\n",
    "        elif config_name == 'wind':\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "        elif config_name == 'noise':\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        elif config_name == 'all':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, land_bonus=80.0)\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        # caso 'orig' => nenhum wrapper\n",
    "\n",
    "        # Monitor: grava os episódios neste ficheiro\n",
    "        if monitor_dir is not None:\n",
    "            os.makedirs(monitor_dir, exist_ok=True)\n",
    "            monitor_file = os.path.join(monitor_dir, 'monitor.csv')\n",
    "            env = Monitor(env, filename=monitor_file)\n",
    "        else:\n",
    "            env = Monitor(env)\n",
    "\n",
    "        return env\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90f4e3",
   "metadata": {},
   "source": [
    "# Função de treino do PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6054db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config(config_name='orig', seed=0, timesteps=300_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Treina um PPO para a configuração especificada.\n",
    "    - Guarda modelo e monitor.csv em out_root/config_name/seedX/\n",
    "    - hyperparams: dict que sobrepõe os defaults do PPO (learning_rate,n_steps,batch_size,n_epochs,...)\n",
    "    \"\"\"\n",
    "    if hyperparams is None:\n",
    "        hyperparams = {}\n",
    "\n",
    "    out_dir = os.path.join(out_root, config_name, f'seed{seed}')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # reproducibilidade\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    # criar env factory (monitor dentro da pasta out_dir)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=out_dir)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    # policy defaults\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[dict(pi=[256,256], vf=[256,256])])\n",
    "\n",
    "    # PPO defaults (podes sobrepor via hyperparams)\n",
    "    ppo_defaults = dict(\n",
    "        policy='MlpPolicy',\n",
    "        env=vec_env,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        learning_rate=3e-5,\n",
    "        n_steps=4096,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.0,\n",
    "        vf_coef=0.5,\n",
    "        clip_range=0.2,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        tensorboard_log=os.path.join(out_dir, 'tb')\n",
    "    )\n",
    "    # update defaults with provided hyperparams\n",
    "    ppo_defaults.update(hyperparams)\n",
    "\n",
    "    model = PPO(**ppo_defaults)\n",
    "\n",
    "    # callbacks: evaluation e checkpoints (guardam em out_dir)\n",
    "    eval_env = DummyVecEnv([make_env_factory(seed=seed+1000, config_name=config_name, monitor_dir=None)])\n",
    "    eval_env = VecMonitor(eval_env)\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=out_dir,\n",
    "                                 log_path=out_dir, eval_freq=max(10_000, ppo_defaults['n_steps']*2),\n",
    "                                 n_eval_episodes=5, deterministic=True, render=False)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=max(50_000, ppo_defaults['n_steps']*5),\n",
    "                                             save_path=out_dir, name_prefix='ppo_checkpoint')\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "    model_path = os.path.join(out_dir, f'ppo_{config_name}_seed{seed}.zip')\n",
    "    model.save(model_path)\n",
    "\n",
    "    vec_env.close()\n",
    "    eval_env.close()\n",
    "    print(f\"[TRAIN] Saved model: {model_path}\")\n",
    "    return model_path, out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bf8b5",
   "metadata": {},
   "source": [
    "# Função de avaliação com critério revisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7120a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_custom(model_path, config_name='orig', seed=None, episodes=50):\n",
    "    \"\"\"\n",
    "    Avalia o modelo (usa DummyVecEnv com mesma config). Critério de sucesso:\n",
    "     - em qualquer step do episódio both legs touched, OR total_reward >= 200\n",
    "    Retorna dicionário com métricas e lista de recompensas por episódio.\n",
    "    \"\"\"\n",
    "    # carregar env (para avaliação, monitor não é necessário)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=None)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    model = PPO.load(model_path, env=vec_env)\n",
    "\n",
    "    # avaliação rápida via evaluate_policy (apenas para ter mean/std)\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=min(10, episodes), deterministic=True)\n",
    "    print(f\"[EVAL] quick evaluate_policy: mean={mean_reward:.2f} std={std_reward:.2f}\")\n",
    "\n",
    "    # per-episode sampling para success metric\n",
    "    successes = 0\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        reset_out = vec_env.reset()\n",
    "        obs = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "\n",
    "        done = False\n",
    "        total_r = 0.0\n",
    "        landed_flag = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            step_out = vec_env.step(action)\n",
    "            # VecEnv.step returns (obs, reward, done, info)\n",
    "            obs, reward, done, info = step_out\n",
    "            # reward pode ser array shape (1,), garantir float\n",
    "            try:\n",
    "                total_r += float(np.array(reward).item())\n",
    "            except Exception:\n",
    "                total_r += float(reward)\n",
    "\n",
    "            # verificar contacto das pernas na observação atual\n",
    "            try:\n",
    "                last_obs = obs[0]  # porque DummyVecEnv usa batch dimension\n",
    "                if bool(last_obs[6]) and bool(last_obs[7]):\n",
    "                    landed_flag = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if landed_flag or total_r >= 200:\n",
    "            successes += 1\n",
    "        rewards.append(total_r)\n",
    "\n",
    "    vec_env.close()\n",
    "    rewards = np.array(rewards)\n",
    "    result = {\n",
    "        'mean_reward': float(rewards.mean()),\n",
    "        'std_reward': float(rewards.std()),\n",
    "        'median_reward': float(np.median(rewards)),\n",
    "        'success_rate': float(successes) / len(rewards),\n",
    "        'per_episode': rewards\n",
    "    }\n",
    "    print(f\"[EVAL] result: mean={result['mean_reward']:.2f} std={result['std_reward']:.2f} success_rate={result['success_rate']:.2f}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795d9f7",
   "metadata": {},
   "source": [
    "# Função para plot de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "069eafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_monitor(monitor_csv_path, window=10, show=True, out_png=None):\n",
    "    \"\"\"Plota rewards (raw + smoothed) a partir do monitor.csv gerado pelo Monitor.\"\"\"\n",
    "    if not os.path.exists(monitor_csv_path):\n",
    "        print(\"[PLOT] monitor file not found:\", monitor_csv_path)\n",
    "        return\n",
    "    df = pd.read_csv(monitor_csv_path, skiprows=1, names=['r','l','t'])\n",
    "    df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(df['r'], alpha=0.25, label='raw')\n",
    "    plt.plot(df['r_smooth'], label=f'smoothed({window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(os.path.basename(os.path.dirname(monitor_csv_path)))\n",
    "    plt.legend()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, bbox_inches='tight')\n",
    "        print(\"[PLOT] saved to\", out_png)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_compare_configs(base_dir, configs, seed, window=20):\n",
    "    \"\"\"Plota curvas suavizadas (por seed) comparando várias configs no mesmo gráfico.\"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for cfg in configs:\n",
    "        monitor_csv = os.path.join(base_dir, cfg, f'seed{seed}', 'monitor.csv')\n",
    "        if not os.path.exists(monitor_csv):\n",
    "            print(\"[COMPARE] monitor not found for\", cfg, \"seed\", seed)\n",
    "            continue\n",
    "        df = pd.read_csv(monitor_csv, skiprows=1, names=['r','l','t'])\n",
    "        df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "        plt.plot(df['r_smooth'], label=f'{cfg}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Smoothed Reward')\n",
    "    plt.title(f'Compare configs (seed {seed})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c532d",
   "metadata": {},
   "source": [
    "# Execução para múltiplas seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ef02740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Roda treinos e avaliações para lista de configs e seeds.\n",
    "    Retorna um dict results[config][seed] = metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for cfg in configs:\n",
    "        results[cfg] = {}\n",
    "        for s in seeds:\n",
    "            print(\"\\n\\n============================\")\n",
    "            print(f\"Training config={cfg} seed={s}\")\n",
    "            model_path, out_dir = train_config(config_name=cfg, seed=s, timesteps=timesteps,\n",
    "                                               hyperparams=hyperparams, out_root=out_root)\n",
    "            print(f\"Evaluating model for config={cfg} seed={s}\")\n",
    "            res = evaluate_custom(model_path=model_path, config_name=cfg, seed=s, episodes=50)\n",
    "            results[cfg][s] = res\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37fd07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def run_hyperparam_grid(config_name, seed, grid, timesteps=300_000, out_root='./experiments_grid'):\n",
    "    \"\"\"\n",
    "    grid: dict of lists, e.g. {'learning_rate':[3e-5,1e-4],'n_steps':[2048,4096]}\n",
    "    Vai gerar todas as combinações, treinar e guardar resultados em out_root/config_name/seed/hparam_i\n",
    "    Retorna lista de (hparam_dict, metrics)\n",
    "    \"\"\"\n",
    "    keys, values = zip(*grid.items())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    results = []\n",
    "    for i, combo in enumerate(combos):\n",
    "        print(f\"\\n---- Grid {i+1}/{len(combos)}: {combo}\")\n",
    "        # colocar cada combo numa subpasta\n",
    "        out_root_combo = os.path.join(out_root, config_name, f'seed{seed}', f'grid_{i}')\n",
    "        os.makedirs(out_root_combo, exist_ok=True)\n",
    "        model_path, _ = train_config(config_name=config_name, seed=seed, timesteps=timesteps,\n",
    "                                    hyperparams=combo, out_root=out_root_combo)\n",
    "        metrics = evaluate_custom(model_path=model_path, config_name=config_name, seed=seed, episodes=30)\n",
    "        results.append((combo, metrics, model_path, out_root_combo))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea4152",
   "metadata": {},
   "source": [
    "## rodar todos os configs para várias seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bbf073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================\n",
      "Training config=orig seed=0\n",
      "Using cpu device\n",
      "Logging to ./experiments/orig/seed0/tb/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonorarreiol/miniforge3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n",
      "/Users/leonorarreiol/miniforge3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 5040     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 88.7         |\n",
      "|    ep_rew_mean          | -175         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2977         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006454224 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000457    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 685          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 1.49e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-373.42 +/- 59.56\n",
      "Episode length: 127.00 +/- 26.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 127          |\n",
      "|    mean_reward          | -373         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007989865 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.00632     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 450          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 1.17e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    fps             | 2489     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.5        |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2347        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001495226 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0412     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 437         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00255    |\n",
      "|    value_loss           | 1.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-427.33 +/- 355.59\n",
      "Episode length: 120.80 +/- 26.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 121          |\n",
      "|    mean_reward          | -427         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022574011 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0837      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 415          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 1.05e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    fps             | 2264     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.2         |\n",
      "|    ep_rew_mean          | -164         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2247         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026775873 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0442      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 361          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 655          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.6        |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2240        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004996667 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.00235    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 509         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-805.17 +/- 473.20\n",
      "Episode length: 347.00 +/- 94.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | -805        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003501519 |\n",
      "|    clip_fraction        | 0.00125     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 289         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 529         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 2186     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -131         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2190         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019142886 |\n",
      "|    clip_fraction        | 0.000317     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.0131      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 175          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 398          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1077.27 +/- 234.16\n",
      "Episode length: 202.80 +/- 38.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 203         |\n",
      "|    mean_reward          | -1.08e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009404298 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.0029      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -116     |\n",
      "| time/              |          |\n",
      "|    fps             | 2174     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 107        |\n",
      "|    ep_rew_mean          | -118       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2175       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00780504 |\n",
      "|    clip_fraction        | 0.0157     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | -0.00192   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 84.2       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0056    |\n",
      "|    value_loss           | 311        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -115         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2182         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011893669 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.0271      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 282          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1166.60 +/- 124.46\n",
      "Episode length: 194.40 +/- 53.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 194        |\n",
      "|    mean_reward          | -1.17e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00802296 |\n",
      "|    clip_fraction        | 0.00442    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -0.00205   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 195        |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00362   |\n",
      "|    value_loss           | 326        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -114     |\n",
      "| time/              |          |\n",
      "|    fps             | 2174     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/orig/seed0/ppo_orig_seed0.zip\n",
      "Evaluating model for config=orig seed=0\n",
      "[EVAL] quick evaluate_policy: mean=-989.67 std=640.20\n",
      "[EVAL] result: mean=-900.56 std=604.71 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=orig seed=7\n",
      "Using cpu device\n",
      "Logging to ./experiments/orig/seed7/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.6     |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    fps             | 5295     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.7         |\n",
      "|    ep_rew_mean          | -165         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3169         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031294054 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.00369      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 680          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 1.55e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-453.27 +/- 157.89\n",
      "Episode length: 162.00 +/- 36.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 162          |\n",
      "|    mean_reward          | -453         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002221147 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0147      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 422          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000415    |\n",
      "|    value_loss           | 1.05e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.7     |\n",
      "|    ep_rew_mean     | -157     |\n",
      "| time/              |          |\n",
      "|    fps             | 2703     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 95            |\n",
      "|    ep_rew_mean          | -151          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2542          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00062061194 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.0739       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 702           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    value_loss           | 851           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-727.38 +/- 123.82\n",
      "Episode length: 204.20 +/- 99.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 204          |\n",
      "|    mean_reward          | -727         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012764486 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0241      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 356          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 787          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    fps             | 2434     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.8        |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2408        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004991264 |\n",
      "|    clip_fraction        | 0.00103     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.00703    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 294         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00353    |\n",
      "|    value_loss           | 680         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | -149         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2393         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055016917 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 228          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 522          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-988.49 +/- 498.34\n",
      "Episode length: 265.40 +/- 37.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 265          |\n",
      "|    mean_reward          | -988         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024387112 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.023       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 318          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 506          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    fps             | 2348     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2345        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010129578 |\n",
      "|    clip_fraction        | 0.0461      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.000936   |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 290         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-907.85 +/- 326.68\n",
      "Episode length: 251.20 +/- 32.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -908        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009703405 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.00205    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 175         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    value_loss           | 468         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 2291     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2290        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002238997 |\n",
      "|    clip_fraction        | 0.000781    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.00139    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 250         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 531         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 141         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2271        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005149955 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -0.0174     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 248         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 436         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-872.73 +/- 152.48\n",
      "Episode length: 266.20 +/- 30.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 266          |\n",
      "|    mean_reward          | -873         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011600086 |\n",
      "|    clip_fraction        | 0.00061      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.00676      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 140          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 415          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | -136     |\n",
      "| time/              |          |\n",
      "|    fps             | 2241     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/orig/seed7/ppo_orig_seed7.zip\n",
      "Evaluating model for config=orig seed=7\n",
      "[EVAL] quick evaluate_policy: mean=-1643.85 std=389.75\n",
      "[EVAL] result: mean=-1604.49 std=433.56 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=orig seed=42\n",
      "Using cpu device\n",
      "Logging to ./experiments/orig/seed42/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    fps             | 5000     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90.1         |\n",
      "|    ep_rew_mean          | -154         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2907         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030425903 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000357    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 519          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 1.21e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-869.10 +/- 473.30\n",
      "Episode length: 132.80 +/- 46.52\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 133           |\n",
      "|    mean_reward          | -869          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084836583 |\n",
      "|    clip_fraction        | 2.44e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.00117       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 681           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00179      |\n",
      "|    value_loss           | 1.17e+03      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.5     |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    fps             | 2490     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.3         |\n",
      "|    ep_rew_mean          | -151         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2360         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008766901 |\n",
      "|    clip_fraction        | 0.000366     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0282      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 402          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 919          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-958.56 +/- 456.97\n",
      "Episode length: 144.80 +/- 56.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 145         |\n",
      "|    mean_reward          | -959        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342785 |\n",
      "|    clip_fraction        | 0.00667     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 502         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 2245     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -124         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2200         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034432656 |\n",
      "|    clip_fraction        | 0.00901      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.0842       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    value_loss           | 494          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2175        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009904895 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 110         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1665.77 +/- 747.56\n",
      "Episode length: 243.20 +/- 42.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 243         |\n",
      "|    mean_reward          | -1.67e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010849849 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.0217     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 163         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    value_loss           | 300         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    fps             | 2131     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 122        |\n",
      "|    ep_rew_mean          | -129       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2123       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01067435 |\n",
      "|    clip_fraction        | 0.00813    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | -0.0914    |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 242        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00377   |\n",
      "|    value_loss           | 362        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1845.93 +/- 857.08\n",
      "Episode length: 261.80 +/- 48.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | -1.85e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002905788 |\n",
      "|    clip_fraction        | 0.00305     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.00333    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 613         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    fps             | 2105     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 151          |\n",
      "|    ep_rew_mean          | -151         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2115         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034476505 |\n",
      "|    clip_fraction        | 0.00847      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00604     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 173          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 386          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | -152         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2124         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031075056 |\n",
      "|    clip_fraction        | 0.00476      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00568     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 183          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 493          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-4230.88 +/- 486.14\n",
      "Episode length: 561.60 +/- 21.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 562          |\n",
      "|    mean_reward          | -4.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060640927 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.00232      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 223          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 408          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 152      |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    fps             | 2084     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/orig/seed42/ppo_orig_seed42.zip\n",
      "Evaluating model for config=orig seed=42\n",
      "[EVAL] quick evaluate_policy: mean=-3345.41 std=1462.77\n",
      "[EVAL] result: mean=-3295.41 std=1495.99 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=reward seed=0\n",
      "Using cpu device\n",
      "Logging to ./experiments/reward/seed0/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 5060     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 88.7         |\n",
      "|    ep_rew_mean          | -175         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2991         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006454224 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000457    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 685          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    value_loss           | 1.49e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-373.42 +/- 59.56\n",
      "Episode length: 127.00 +/- 26.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 127          |\n",
      "|    mean_reward          | -373         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007989865 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.00632     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 450          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 1.17e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    fps             | 2481     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.5        |\n",
      "|    ep_rew_mean          | -174        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2387        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001495226 |\n",
      "|    clip_fraction        | 0.000146    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0412     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 437         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00255    |\n",
      "|    value_loss           | 1.09e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-427.33 +/- 355.59\n",
      "Episode length: 120.80 +/- 26.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 121          |\n",
      "|    mean_reward          | -427         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022574011 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0837      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 415          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 1.05e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    fps             | 2335     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.2         |\n",
      "|    ep_rew_mean          | -164         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2316         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026775873 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0442      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 361          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 655          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 98.6        |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2303        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004996667 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.00235    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 509         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-805.17 +/- 473.20\n",
      "Episode length: 347.00 +/- 94.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | -805        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003501519 |\n",
      "|    clip_fraction        | 0.00125     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 289         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 529         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 2247     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -131         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2250         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019142886 |\n",
      "|    clip_fraction        | 0.000317     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.0131      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 175          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 398          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1077.27 +/- 234.16\n",
      "Episode length: 202.80 +/- 38.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 203         |\n",
      "|    mean_reward          | -1.08e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009404298 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.0029      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -116     |\n",
      "| time/              |          |\n",
      "|    fps             | 2223     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 107        |\n",
      "|    ep_rew_mean          | -118       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2218       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00780504 |\n",
      "|    clip_fraction        | 0.0157     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | -0.00192   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 84.2       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0056    |\n",
      "|    value_loss           | 311        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -115         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2213         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011893669 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.0271      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 282          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1166.60 +/- 124.46\n",
      "Episode length: 194.40 +/- 53.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 194        |\n",
      "|    mean_reward          | -1.17e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00802296 |\n",
      "|    clip_fraction        | 0.00442    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -0.00205   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 195        |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00362   |\n",
      "|    value_loss           | 326        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -114     |\n",
      "| time/              |          |\n",
      "|    fps             | 2195     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/reward/seed0/ppo_reward_seed0.zip\n",
      "Evaluating model for config=reward seed=0\n",
      "[EVAL] quick evaluate_policy: mean=-989.67 std=640.20\n",
      "[EVAL] result: mean=-900.56 std=604.71 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=reward seed=7\n",
      "Using cpu device\n",
      "Logging to ./experiments/reward/seed7/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.6     |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    fps             | 5133     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.7         |\n",
      "|    ep_rew_mean          | -165         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3108         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031294054 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.00369      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 680          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00436     |\n",
      "|    value_loss           | 1.55e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-453.27 +/- 157.89\n",
      "Episode length: 162.00 +/- 36.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 162          |\n",
      "|    mean_reward          | -453         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002221147 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0147      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 422          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000415    |\n",
      "|    value_loss           | 1.05e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.7     |\n",
      "|    ep_rew_mean     | -157     |\n",
      "| time/              |          |\n",
      "|    fps             | 2665     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 95            |\n",
      "|    ep_rew_mean          | -151          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2513          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00062061194 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.0739       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 702           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    value_loss           | 851           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-727.38 +/- 123.82\n",
      "Episode length: 204.20 +/- 99.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 204          |\n",
      "|    mean_reward          | -727         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012764486 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0241      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 356          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 787          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.5     |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    fps             | 2399     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.8        |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2373        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004991264 |\n",
      "|    clip_fraction        | 0.00103     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.00703    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 294         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00353    |\n",
      "|    value_loss           | 680         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 104          |\n",
      "|    ep_rew_mean          | -149         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2349         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055016917 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 228          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00419     |\n",
      "|    value_loss           | 522          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-988.49 +/- 498.34\n",
      "Episode length: 265.40 +/- 37.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 265          |\n",
      "|    mean_reward          | -988         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024387112 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.023       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 318          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 506          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    fps             | 2296     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 112         |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2287        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010129578 |\n",
      "|    clip_fraction        | 0.0461      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.000936   |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 290         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-907.85 +/- 326.68\n",
      "Episode length: 251.20 +/- 32.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | -908        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009703405 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.00205    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 175         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    value_loss           | 468         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 2253     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 128         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2248        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002238997 |\n",
      "|    clip_fraction        | 0.000781    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.00139    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 250         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 531         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 141         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2242        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005149955 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -0.0174     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 248         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 436         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-872.73 +/- 152.48\n",
      "Episode length: 266.20 +/- 30.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 266          |\n",
      "|    mean_reward          | -873         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011600086 |\n",
      "|    clip_fraction        | 0.00061      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.00676      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 140          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 415          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | -136     |\n",
      "| time/              |          |\n",
      "|    fps             | 2218     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/reward/seed7/ppo_reward_seed7.zip\n",
      "Evaluating model for config=reward seed=7\n",
      "[EVAL] quick evaluate_policy: mean=-1643.85 std=389.75\n",
      "[EVAL] result: mean=-1604.49 std=433.56 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=reward seed=42\n",
      "Using cpu device\n",
      "Logging to ./experiments/reward/seed42/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    fps             | 5086     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90.1         |\n",
      "|    ep_rew_mean          | -154         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3019         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030425903 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000357    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 519          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00335     |\n",
      "|    value_loss           | 1.21e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-869.10 +/- 473.30\n",
      "Episode length: 132.80 +/- 46.52\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 133           |\n",
      "|    mean_reward          | -869          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084836583 |\n",
      "|    clip_fraction        | 2.44e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.00117       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 681           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00179      |\n",
      "|    value_loss           | 1.17e+03      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.5     |\n",
      "|    ep_rew_mean     | -153     |\n",
      "| time/              |          |\n",
      "|    fps             | 2553     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.3         |\n",
      "|    ep_rew_mean          | -151         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2441         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008766901 |\n",
      "|    clip_fraction        | 0.000366     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0282      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 402          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 919          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-958.56 +/- 456.97\n",
      "Episode length: 144.80 +/- 56.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 145         |\n",
      "|    mean_reward          | -959        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342785 |\n",
      "|    clip_fraction        | 0.00667     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 502         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 2353     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 116          |\n",
      "|    ep_rew_mean          | -124         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2327         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034432656 |\n",
      "|    clip_fraction        | 0.00901      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.0842       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00404     |\n",
      "|    value_loss           | 494          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2305        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009904895 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 110         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1665.77 +/- 747.56\n",
      "Episode length: 243.20 +/- 42.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 243         |\n",
      "|    mean_reward          | -1.67e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010849849 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.0217     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 163         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    value_loss           | 300         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    fps             | 2265     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 122        |\n",
      "|    ep_rew_mean          | -129       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2265       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01067435 |\n",
      "|    clip_fraction        | 0.00813    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | -0.0914    |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 242        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00377   |\n",
      "|    value_loss           | 362        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1845.93 +/- 857.08\n",
      "Episode length: 261.80 +/- 48.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | -1.85e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002905788 |\n",
      "|    clip_fraction        | 0.00305     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.00333    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 613         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    fps             | 2225     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 151          |\n",
      "|    ep_rew_mean          | -151         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2210         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034476505 |\n",
      "|    clip_fraction        | 0.00847      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00604     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 173          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 386          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 156          |\n",
      "|    ep_rew_mean          | -152         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2207         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031075056 |\n",
      "|    clip_fraction        | 0.00476      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00568     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 183          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 493          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-4230.88 +/- 486.14\n",
      "Episode length: 561.60 +/- 21.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 562          |\n",
      "|    mean_reward          | -4.23e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060640927 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.00232      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 223          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    value_loss           | 408          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 152      |\n",
      "|    ep_rew_mean     | -134     |\n",
      "| time/              |          |\n",
      "|    fps             | 2160     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/reward/seed42/ppo_reward_seed42.zip\n",
      "Evaluating model for config=reward seed=42\n",
      "[EVAL] quick evaluate_policy: mean=-3345.41 std=1462.77\n",
      "[EVAL] result: mean=-3295.41 std=1495.99 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=wind seed=0\n",
      "Using cpu device\n",
      "Logging to ./experiments/wind/seed0/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 5124     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90.1         |\n",
      "|    ep_rew_mean          | -194         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3016         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005637683 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000445    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 571          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-431.84 +/- 182.29\n",
      "Episode length: 124.00 +/- 48.39\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 124           |\n",
      "|    mean_reward          | -432          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092646317 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.00809       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 604           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00196      |\n",
      "|    value_loss           | 1.49e+03      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    fps             | 2567     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 99.8         |\n",
      "|    ep_rew_mean          | -177         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2434         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023310399 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0985      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 538          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00373     |\n",
      "|    value_loss           | 1.41e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-344.36 +/- 217.57\n",
      "Episode length: 122.60 +/- 36.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 123          |\n",
      "|    mean_reward          | -344         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011497162 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.114        |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 348          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00211     |\n",
      "|    value_loss           | 734          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -158     |\n",
      "| time/              |          |\n",
      "|    fps             | 2336     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 107          |\n",
      "|    ep_rew_mean          | -148         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2303         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028884306 |\n",
      "|    clip_fraction        | 0.000269     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0821      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 322          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 692          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 99.4         |\n",
      "|    ep_rew_mean          | -144         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2262         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012459857 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0588      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 179          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 564          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-665.19 +/- 158.31\n",
      "Episode length: 129.40 +/- 23.94\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 129          |\n",
      "|    mean_reward          | -665         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055924123 |\n",
      "|    clip_fraction        | 0.00618      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.0219      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 423          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    value_loss           | 510          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 2233     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2222        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008350197 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 364         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-733.76 +/- 239.44\n",
      "Episode length: 142.60 +/- 26.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 143          |\n",
      "|    mean_reward          | -734         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050628595 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | -0.0118      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 108          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    value_loss           | 292          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 2197     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2192        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007303261 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -0.00197    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 266         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -130         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2188         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021225126 |\n",
      "|    clip_fraction        | 0.00022      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.00142      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 322          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1272.72 +/- 372.10\n",
      "Episode length: 234.00 +/- 48.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 234         |\n",
      "|    mean_reward          | -1.27e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010543792 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.021      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00819    |\n",
      "|    value_loss           | 272         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -135     |\n",
      "| time/              |          |\n",
      "|    fps             | 2170     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/wind/seed0/ppo_wind_seed0.zip\n",
      "Evaluating model for config=wind seed=0\n",
      "[EVAL] quick evaluate_policy: mean=-1631.81 std=704.52\n",
      "[EVAL] result: mean=-1555.34 std=449.88 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=wind seed=7\n",
      "Using cpu device\n",
      "Logging to ./experiments/wind/seed7/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.6     |\n",
      "|    ep_rew_mean     | -176     |\n",
      "| time/              |          |\n",
      "|    fps             | 5117     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.7         |\n",
      "|    ep_rew_mean          | -176         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2947         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030147552 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.00372      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 785          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    value_loss           | 1.59e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-226.56 +/- 66.60\n",
      "Episode length: 179.80 +/- 59.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 180          |\n",
      "|    mean_reward          | -227         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010117162 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0241      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 500          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 1.11e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.5     |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    fps             | 2457     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.9         |\n",
      "|    ep_rew_mean          | -167         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2327         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011507191 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0239      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 242          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 765          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-223.33 +/- 195.24\n",
      "Episode length: 108.60 +/- 47.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 109          |\n",
      "|    mean_reward          | -223         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007683777 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.089       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 460          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 1.03e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.6     |\n",
      "|    ep_rew_mean     | -159     |\n",
      "| time/              |          |\n",
      "|    fps             | 2250     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2225        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002161097 |\n",
      "|    clip_fraction        | 0.000293    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.0112      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 439         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    value_loss           | 549         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2197        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006857632 |\n",
      "|    clip_fraction        | 0.0246      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0944      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 341         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1150.69 +/- 435.75\n",
      "Episode length: 284.60 +/- 55.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | -1.15e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001610896 |\n",
      "|    clip_fraction        | 2.44e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.00924    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 463         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -149     |\n",
      "| time/              |          |\n",
      "|    fps             | 2136     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 113         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2125        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004587678 |\n",
      "|    clip_fraction        | 0.00327     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.0748     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 261         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    value_loss           | 527         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-751.19 +/- 253.23\n",
      "Episode length: 335.00 +/- 82.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 335        |\n",
      "|    mean_reward          | -751       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00921257 |\n",
      "|    clip_fraction        | 0.0322     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | 0.084      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 137        |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00719   |\n",
      "|    value_loss           | 249        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    fps             | 2092     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2090        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412179 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.0431     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 187         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00529    |\n",
      "|    value_loss           | 307         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 119         |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2090        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007919237 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 123         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1932.12 +/- 430.48\n",
      "Episode length: 445.20 +/- 33.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 445         |\n",
      "|    mean_reward          | -1.93e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004079198 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -0.00658    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 136         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    value_loss           | 355         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/wind/seed7/ppo_wind_seed7.zip\n",
      "Evaluating model for config=wind seed=7\n",
      "[EVAL] quick evaluate_policy: mean=-1921.23 std=477.27\n",
      "[EVAL] result: mean=-1903.00 std=552.81 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=wind seed=42\n",
      "Using cpu device\n",
      "Logging to ./experiments/wind/seed42/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    fps             | 5064     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 92.6        |\n",
      "|    ep_rew_mean          | -173        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3017        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002896521 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | -0.000346   |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 450         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 1.25e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-556.55 +/- 79.34\n",
      "Episode length: 97.00 +/- 10.71\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 97            |\n",
      "|    mean_reward          | -557          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 10000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088244077 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.0131       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 1e+03         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00184      |\n",
      "|    value_loss           | 1.49e+03      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -175     |\n",
      "| time/              |          |\n",
      "|    fps             | 2647     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | -188        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2532        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000871839 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 412         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 825         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-1024.91 +/- 560.16\n",
      "Episode length: 157.00 +/- 54.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 157          |\n",
      "|    mean_reward          | -1.02e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019338686 |\n",
      "|    clip_fraction        | 0.00129      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0444      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 825          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00383     |\n",
      "|    value_loss           | 914          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | -176     |\n",
      "| time/              |          |\n",
      "|    fps             | 2410     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 106         |\n",
      "|    ep_rew_mean          | -160        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2361        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003132836 |\n",
      "|    clip_fraction        | 0.000488    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00776    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 360         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00291    |\n",
      "|    value_loss           | 597         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 109          |\n",
      "|    ep_rew_mean          | -155         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2306         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047927415 |\n",
      "|    clip_fraction        | 0.0011       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.0155      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 359          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 505          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1150.90 +/- 591.20\n",
      "Episode length: 214.60 +/- 38.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 215         |\n",
      "|    mean_reward          | -1.15e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005195743 |\n",
      "|    clip_fraction        | 0.00625     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -0.0259     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 302         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00457    |\n",
      "|    value_loss           | 548         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | -160     |\n",
      "| time/              |          |\n",
      "|    fps             | 2251     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 117          |\n",
      "|    ep_rew_mean          | -148         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2250         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041893525 |\n",
      "|    clip_fraction        | 0.00276      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.0131      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 119          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 469          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1343.11 +/- 777.06\n",
      "Episode length: 287.40 +/- 47.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 287         |\n",
      "|    mean_reward          | -1.34e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010226548 |\n",
      "|    clip_fraction        | 0.0605      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.00946    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 191         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 417         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    fps             | 2197     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 129         |\n",
      "|    ep_rew_mean          | -146        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2188        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007307668 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -0.043      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 571         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 148         |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2186        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009062178 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -0.0017     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 221         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00591    |\n",
      "|    value_loss           | 458         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1761.05 +/- 556.24\n",
      "Episode length: 289.40 +/- 45.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 289         |\n",
      "|    mean_reward          | -1.76e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005069526 |\n",
      "|    clip_fraction        | 0.00259     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0036      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 144         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    value_loss           | 396         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 165      |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    fps             | 2158     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/wind/seed42/ppo_wind_seed42.zip\n",
      "Evaluating model for config=wind seed=42\n",
      "[EVAL] quick evaluate_policy: mean=-1634.30 std=552.79\n",
      "[EVAL] result: mean=-1701.76 std=704.98 success_rate=0.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=noise seed=0\n",
      "Using cpu device\n",
      "Logging to ./experiments/noise/seed0/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 4789     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 89            |\n",
      "|    ep_rew_mean          | -169          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2887          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066587114 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -0.000462     |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 348           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00152      |\n",
      "|    value_loss           | 1.49e+03      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-643.56 +/- 61.70\n",
      "Episode length: 97.40 +/- 11.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 97.4         |\n",
      "|    mean_reward          | -644         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007778928 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0105      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 448          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 1.12e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.7     |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    fps             | 2493     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.5         |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2379         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030828784 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0395      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 737          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    value_loss           | 1.35e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-616.62 +/- 248.08\n",
      "Episode length: 149.20 +/- 71.32\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 149          |\n",
      "|    mean_reward          | -617         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030543809 |\n",
      "|    clip_fraction        | 0.00166      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0432      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 317          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 781          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.3     |\n",
      "|    ep_rew_mean     | -175     |\n",
      "| time/              |          |\n",
      "|    fps             | 2271     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | -166         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2216         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039781146 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.059       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 172          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 879          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | -161        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2196        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009048618 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0268     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 215         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    value_loss           | 532         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1112.44 +/- 854.53\n",
      "Episode length: 169.80 +/- 64.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 170          |\n",
      "|    mean_reward          | -1.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030614939 |\n",
      "|    clip_fraction        | 0.00481      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.016        |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 263          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    value_loss           | 618          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | -161     |\n",
      "| time/              |          |\n",
      "|    fps             | 2152     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | -151         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2148         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072039696 |\n",
      "|    clip_fraction        | 0.0095       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -0.145       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 138          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 538          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1072.62 +/- 164.55\n",
      "Episode length: 155.00 +/- 28.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 155         |\n",
      "|    mean_reward          | -1.07e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003197432 |\n",
      "|    clip_fraction        | 0.031       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.024      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 144         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    value_loss           | 345         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | -147     |\n",
      "| time/              |          |\n",
      "|    fps             | 2136     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 133         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2124        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003292935 |\n",
      "|    clip_fraction        | 0.002       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.0426      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00298    |\n",
      "|    value_loss           | 305         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 143         |\n",
      "|    ep_rew_mean          | -131        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2115        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008495889 |\n",
      "|    clip_fraction        | 0.0207      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.00393    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 151         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00517    |\n",
      "|    value_loss           | 300         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1484.52 +/- 720.18\n",
      "Episode length: 215.40 +/- 41.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 215        |\n",
      "|    mean_reward          | -1.48e+03  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00665601 |\n",
      "|    clip_fraction        | 0.0168     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | -0.0153    |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 159        |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00598   |\n",
      "|    value_loss           | 335        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    fps             | 2099     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/noise/seed0/ppo_noise_seed0.zip\n",
      "Evaluating model for config=noise seed=0\n",
      "[EVAL] quick evaluate_policy: mean=-1579.72 std=429.51\n",
      "[EVAL] result: mean=-1884.12 std=1094.48 success_rate=1.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=noise seed=7\n",
      "Using cpu device\n",
      "Logging to ./experiments/noise/seed7/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.7     |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    fps             | 5076     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 91.7         |\n",
      "|    ep_rew_mean          | -149         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2874         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031325116 |\n",
      "|    clip_fraction        | 0.00208      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00417      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 556          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00561     |\n",
      "|    value_loss           | 1.5e+03      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-1105.23 +/- 619.84\n",
      "Episode length: 276.60 +/- 56.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 277          |\n",
      "|    mean_reward          | -1.11e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011525847 |\n",
      "|    clip_fraction        | 2.44e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.011        |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 277          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    value_loss           | 825          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93       |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    fps             | 2400     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 96.3          |\n",
      "|    ep_rew_mean          | -138          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2320          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096611166 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.00103      |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 343           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0011       |\n",
      "|    value_loss           | 718           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-1506.50 +/- 979.33\n",
      "Episode length: 186.80 +/- 85.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 187          |\n",
      "|    mean_reward          | -1.51e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029977323 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0151      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 243          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 671          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.5     |\n",
      "|    ep_rew_mean     | -147     |\n",
      "| time/              |          |\n",
      "|    fps             | 2255     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | -154         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2235         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008962733 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.053       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 366          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    value_loss           | 813          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2224        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002075363 |\n",
      "|    clip_fraction        | 0.000122    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 574         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1080.46 +/- 447.29\n",
      "Episode length: 428.20 +/- 97.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 428         |\n",
      "|    mean_reward          | -1.08e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003362461 |\n",
      "|    clip_fraction        | 0.00256     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    value_loss           | 497         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | -129     |\n",
      "| time/              |          |\n",
      "|    fps             | 2148     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 107         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2153        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010809278 |\n",
      "|    clip_fraction        | 0.0226      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.0127     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 156         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    value_loss           | 345         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-878.51 +/- 294.58\n",
      "Episode length: 216.80 +/- 19.14\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 217           |\n",
      "|    mean_reward          | -879          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049967255 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.31         |\n",
      "|    explained_variance   | -0.0126       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 313           |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    value_loss           | 671           |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    fps             | 2127     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 117         |\n",
      "|    ep_rew_mean          | -148        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2115        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010583287 |\n",
      "|    clip_fraction        | 0.0191      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.00582    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 355         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00623    |\n",
      "|    value_loss           | 459         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -125         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2114         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065540187 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 2.12e-05     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 217          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00823     |\n",
      "|    value_loss           | 431          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-914.70 +/- 407.12\n",
      "Episode length: 220.80 +/- 43.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 221        |\n",
      "|    mean_reward          | -915       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00677743 |\n",
      "|    clip_fraction        | 0.01       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | -0.00234   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 134        |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00466   |\n",
      "|    value_loss           | 371        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 140      |\n",
      "|    ep_rew_mean     | -121     |\n",
      "| time/              |          |\n",
      "|    fps             | 2094     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/noise/seed7/ppo_noise_seed7.zip\n",
      "Evaluating model for config=noise seed=7\n",
      "[EVAL] quick evaluate_policy: mean=-2186.65 std=449.69\n",
      "[EVAL] result: mean=-2117.56 std=848.65 success_rate=1.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=noise seed=42\n",
      "Using cpu device\n",
      "Logging to ./experiments/noise/seed42/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.5     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 4922     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 91.4         |\n",
      "|    ep_rew_mean          | -177         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2924         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008583218 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.00138     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 583          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    value_loss           | 1.62e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-2281.08 +/- 1370.75\n",
      "Episode length: 256.20 +/- 89.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 256          |\n",
      "|    mean_reward          | -2.28e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013280923 |\n",
      "|    clip_fraction        | 0.000659     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0556      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 509          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    value_loss           | 1.36e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.5     |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    fps             | 2487     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 89.7         |\n",
      "|    ep_rew_mean          | -150         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2355         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013881841 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0244      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 587          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00237     |\n",
      "|    value_loss           | 895          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-2245.64 +/- 503.89\n",
      "Episode length: 320.80 +/- 60.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | -2.25e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054810513 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0648      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 456          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    value_loss           | 744          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95       |\n",
      "|    ep_rew_mean     | -148     |\n",
      "| time/              |          |\n",
      "|    fps             | 2241     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 103          |\n",
      "|    ep_rew_mean          | -149         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2222         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010855274 |\n",
      "|    clip_fraction        | 2.44e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.0304      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 232          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 929          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -143        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2202        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005442609 |\n",
      "|    clip_fraction        | 0.00845     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0906     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 240         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 533         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-900.92 +/- 593.17\n",
      "Episode length: 163.80 +/- 70.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 164         |\n",
      "|    mean_reward          | -901        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659937 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.0395     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 123         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    value_loss           | 491         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    fps             | 2176     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 117        |\n",
      "|    ep_rew_mean          | -142       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2163       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00812519 |\n",
      "|    clip_fraction        | 0.0464     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.31      |\n",
      "|    explained_variance   | -0.00495   |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 184        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00527   |\n",
      "|    value_loss           | 365        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1286.34 +/- 263.64\n",
      "Episode length: 225.20 +/- 25.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | -1.29e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035012148 |\n",
      "|    clip_fraction        | 0.0032       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -0.0103      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    value_loss           | 545          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | -131     |\n",
      "| time/              |          |\n",
      "|    fps             | 2130     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 133          |\n",
      "|    ep_rew_mean          | -126         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2131         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084826825 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | -0.00147     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 161          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00473     |\n",
      "|    value_loss           | 298          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 142          |\n",
      "|    ep_rew_mean          | -118         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2132         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051850174 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | -0.00034     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 160          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    value_loss           | 214          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1127.04 +/- 378.81\n",
      "Episode length: 288.00 +/- 30.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 288          |\n",
      "|    mean_reward          | -1.13e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020050947 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00344     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 135          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 308          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 152      |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    fps             | 2114     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/noise/seed42/ppo_noise_seed42.zip\n",
      "Evaluating model for config=noise seed=42\n",
      "[EVAL] quick evaluate_policy: mean=-2000.39 std=1000.94\n",
      "[EVAL] result: mean=-2937.32 std=1650.53 success_rate=1.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=all seed=0\n",
      "Using cpu device\n",
      "Logging to ./experiments/all/seed0/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -180     |\n",
      "| time/              |          |\n",
      "|    fps             | 4742     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90.9         |\n",
      "|    ep_rew_mean          | -176         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2919         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006220072 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000457    |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 456          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 1.53e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-443.78 +/- 47.50\n",
      "Episode length: 103.80 +/- 21.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 104          |\n",
      "|    mean_reward          | -444         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006836111 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.00118      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 532          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | -181     |\n",
      "| time/              |          |\n",
      "|    fps             | 2528     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | -179         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2449         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021243612 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0366      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 448          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    value_loss           | 1.27e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-514.77 +/- 177.02\n",
      "Episode length: 170.60 +/- 57.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 171          |\n",
      "|    mean_reward          | -515         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019693743 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0297      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 636          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 1.01e+03     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.1     |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    fps             | 2329     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.4         |\n",
      "|    ep_rew_mean          | -158         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2291         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027208766 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0363      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 231          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 717          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.3         |\n",
      "|    ep_rew_mean          | -142         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2270         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047626626 |\n",
      "|    clip_fraction        | 0.00129      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0386      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 206          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    value_loss           | 616          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1002.57 +/- 473.10\n",
      "Episode length: 326.40 +/- 115.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 326         |\n",
      "|    mean_reward          | -1e+03      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008533163 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.003      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    value_loss           | 431         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -145     |\n",
      "| time/              |          |\n",
      "|    fps             | 2211     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 105         |\n",
      "|    ep_rew_mean          | -141        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2213        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003512459 |\n",
      "|    clip_fraction        | 0.000415    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.00706    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 452         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    value_loss           | 469         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1137.82 +/- 218.51\n",
      "Episode length: 214.80 +/- 92.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 215          |\n",
      "|    mean_reward          | -1.14e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038471213 |\n",
      "|    clip_fraction        | 0.00671      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | -0.012       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 145          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    value_loss           | 468          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    fps             | 2197     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2197        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007836141 |\n",
      "|    clip_fraction        | 0.0304      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.00152    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 365         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 140         |\n",
      "|    ep_rew_mean          | -144        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2196        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007169797 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.0119     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-2366.46 +/- 1121.86\n",
      "Episode length: 291.40 +/- 76.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 291          |\n",
      "|    mean_reward          | -2.37e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059159826 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | -0.00129     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 133          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00559     |\n",
      "|    value_loss           | 337          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | -128     |\n",
      "| time/              |          |\n",
      "|    fps             | 2172     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/all/seed0/ppo_all_seed0.zip\n",
      "Evaluating model for config=all seed=0\n",
      "[EVAL] quick evaluate_policy: mean=-1964.27 std=808.10\n",
      "[EVAL] result: mean=-2053.70 std=788.98 success_rate=1.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=all seed=7\n",
      "Using cpu device\n",
      "Logging to ./experiments/all/seed7/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.6     |\n",
      "|    ep_rew_mean     | -176     |\n",
      "| time/              |          |\n",
      "|    fps             | 4966     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.9         |\n",
      "|    ep_rew_mean          | -178         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3048         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031269556 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0037       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 587          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    value_loss           | 1.59e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-100.42 +/- 64.91\n",
      "Episode length: 172.60 +/- 38.16\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 173          |\n",
      "|    mean_reward          | -100         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012567337 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 664          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94       |\n",
      "|    ep_rew_mean     | -177     |\n",
      "| time/              |          |\n",
      "|    fps             | 2604     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 92.4          |\n",
      "|    ep_rew_mean          | -163          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2505          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025866344 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.0363       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 462           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000513     |\n",
      "|    value_loss           | 1.01e+03      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-325.40 +/- 162.52\n",
      "Episode length: 268.20 +/- 60.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 268          |\n",
      "|    mean_reward          | -325         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016060108 |\n",
      "|    clip_fraction        | 0.000146     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.036       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 284          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 741          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -160     |\n",
      "| time/              |          |\n",
      "|    fps             | 2372     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | -154         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2331         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014314493 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.0228       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 206          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    value_loss           | 746          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 102          |\n",
      "|    ep_rew_mean          | -160         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2307         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015628922 |\n",
      "|    clip_fraction        | 7.32e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.00838     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 206          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 473          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-751.95 +/- 167.14\n",
      "Episode length: 205.60 +/- 65.23\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 206           |\n",
      "|    mean_reward          | -752          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 30000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00093886035 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | -0.012        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 218           |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    value_loss           | 591           |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -138     |\n",
      "| time/              |          |\n",
      "|    fps             | 2263     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | -134        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2248        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007119665 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0548     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 266         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    value_loss           | 406         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-610.23 +/- 109.86\n",
      "Episode length: 168.00 +/- 44.99\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 168          |\n",
      "|    mean_reward          | -610         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011781345 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.0477       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 239          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    value_loss           | 414          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | -132     |\n",
      "| time/              |          |\n",
      "|    fps             | 2225     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 119         |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2216        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009735546 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -0.033      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00716    |\n",
      "|    value_loss           | 387         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 121         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2211        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005454645 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.0267     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1216.91 +/- 462.01\n",
      "Episode length: 234.80 +/- 36.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 235         |\n",
      "|    mean_reward          | -1.22e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006417451 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | -0.0233     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 133         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    value_loss           | 470         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -124     |\n",
      "| time/              |          |\n",
      "|    fps             | 2186     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/all/seed7/ppo_all_seed7.zip\n",
      "Evaluating model for config=all seed=7\n",
      "[EVAL] quick evaluate_policy: mean=-1079.78 std=333.64\n",
      "[EVAL] result: mean=-1212.29 std=287.66 success_rate=1.00\n",
      "\n",
      "\n",
      "============================\n",
      "Training config=all seed=42\n",
      "Using cpu device\n",
      "Logging to ./experiments/all/seed42/tb/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.5     |\n",
      "|    ep_rew_mean     | -181     |\n",
      "| time/              |          |\n",
      "|    fps             | 4988     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93.4         |\n",
      "|    ep_rew_mean          | -185         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2980         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008294771 |\n",
      "|    clip_fraction        | 0.000513     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.00138     |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 715          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 1.67e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-667.23 +/- 661.93\n",
      "Episode length: 117.80 +/- 64.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 118         |\n",
      "|    mean_reward          | -667        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000463804 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0649     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 820         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    value_loss           | 1.5e+03     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.1     |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 2568     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 92.8          |\n",
      "|    ep_rew_mean          | -167          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 2429          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023214024 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -0.0671       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 340           |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000958     |\n",
      "|    value_loss           | 839           |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-855.04 +/- 463.88\n",
      "Episode length: 277.80 +/- 90.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 278          |\n",
      "|    mean_reward          | -855         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012782458 |\n",
      "|    clip_fraction        | 2.44e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0102      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 340          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 906          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.3     |\n",
      "|    ep_rew_mean     | -170     |\n",
      "| time/              |          |\n",
      "|    fps             | 2303     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 95.8         |\n",
      "|    ep_rew_mean          | -178         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2273         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012343961 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.0367      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 409          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    value_loss           | 925          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.8         |\n",
      "|    ep_rew_mean          | -173         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2248         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054942286 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0228      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 279          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00462     |\n",
      "|    value_loss           | 718          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-1597.84 +/- 383.16\n",
      "Episode length: 234.20 +/- 36.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 234         |\n",
      "|    mean_reward          | -1.6e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004989753 |\n",
      "|    clip_fraction        | 0.0155      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00541    |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 214         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    value_loss           | 631         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | -156     |\n",
      "| time/              |          |\n",
      "|    fps             | 2205     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 103         |\n",
      "|    ep_rew_mean          | -145        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2197        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010968205 |\n",
      "|    clip_fraction        | 0.0207      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0246     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-1671.44 +/- 1011.05\n",
      "Episode length: 383.60 +/- 245.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 384         |\n",
      "|    mean_reward          | -1.67e+03   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005357992 |\n",
      "|    clip_fraction        | 0.0441      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.0605     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 151         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00377    |\n",
      "|    value_loss           | 392         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | -144     |\n",
      "| time/              |          |\n",
      "|    fps             | 2154     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -139        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2148        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002303056 |\n",
      "|    clip_fraction        | 0.000415    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.0162     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 208         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    value_loss           | 439         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 113         |\n",
      "|    ep_rew_mean          | -126        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2147        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005322839 |\n",
      "|    clip_fraction        | 0.00371     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.0026     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    value_loss           | 346         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-1879.08 +/- 498.17\n",
      "Episode length: 276.00 +/- 23.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 276          |\n",
      "|    mean_reward          | -1.88e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040321453 |\n",
      "|    clip_fraction        | 0.00481      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | -0.0071      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 233          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    value_loss           | 264          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | -127     |\n",
      "| time/              |          |\n",
      "|    fps             | 2115     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/all/seed42/ppo_all_seed42.zip\n",
      "Evaluating model for config=all seed=42\n",
      "[EVAL] quick evaluate_policy: mean=-1985.84 std=1158.74\n",
      "[EVAL] result: mean=-1731.62 std=712.05 success_rate=1.00\n"
     ]
    }
   ],
   "source": [
    "configs = ['orig','reward','wind','noise','all']\n",
    "seeds = [0, 7, 42]\n",
    "results = run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc1cf0",
   "metadata": {},
   "source": [
    "## rodar um grid de hyperparams para a config 'all', seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53244fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'learning_rate':[3e-5,1e-4], 'n_steps':[2048,4096]}\n",
    "grid_results = run_hyperparam_grid('all', seed=0, grid=grid, timesteps=300_000, out_root='./experiments_grid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f19e4d",
   "metadata": {},
   "source": [
    "## plot por seed comparando configs (após correr run_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_compare_configs(base_dir='./experiments', configs=['orig','reward','all'], seed=0, window=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d9a84",
   "metadata": {},
   "source": [
    "## plot training monitor de uma pasta concreta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_monitor('./experiments/all/seed0/monitor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336048ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a76166",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
