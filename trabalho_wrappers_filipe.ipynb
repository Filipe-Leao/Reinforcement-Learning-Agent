{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ee9226",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4cf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium e wrappers\n",
    "import gymnasium as gym\n",
    "\n",
    "# Stable-Baselines3\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f69d3",
   "metadata": {},
   "source": [
    "# Wrappers para customizações do LunarLander-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22601665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Reward shaping baseado nas observações do LunarLander.\n",
    "    Obs: [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\n",
    "    \"\"\"\n",
    "    def __init__(self, env, angle_penalty=20.0, dist_penalty=0.5, height_penalty=1.0, land_bonus=100.0):\n",
    "        super().__init__(env)\n",
    "        self.angle_penalty = angle_penalty\n",
    "        self.dist_penalty = dist_penalty\n",
    "        self.land_bonus = land_bonus\n",
    "        self.height_penalty = height_penalty\n",
    "        print(f\"[RewardShapingWrapper] Exponential penalties: height_penalty={height_penalty}, dist_penalty={dist_penalty}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Calcular reward shaping baseado na observação\n",
    "        shaping = 0.0\n",
    "        \n",
    "        try:\n",
    "            # Penalizar por quantidade de steps (incentivar aterragem rápida)\n",
    "            shaping -= 0.1\n",
    "\n",
    "\n",
    "            # LunarLander obs: [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\n",
    "            x = float(obs[0])\n",
    "            y = float(obs[1])\n",
    "            angle = float(obs[4])\n",
    "            angular_vel = float(obs[5])\n",
    "            leg1_contact = bool(obs[6])\n",
    "            leg2_contact = bool(obs[7])\n",
    "            \n",
    "            # BÔNUS: Aterrar com ambas as pernas\n",
    "            if leg1_contact and leg2_contact:\n",
    "                shaping += self.land_bonus\n",
    "\n",
    "            # PENALIZAR: Ângulo e velocidade angular (queremos horizontal e estável)\n",
    "            shaping -= self.angle_penalty * (abs(angle) + abs(angular_vel))\n",
    "\n",
    "            # PENALIZAR: Distância horizontal ao centro (x=0 é o helipad)\n",
    "            # Penalização EXPONENCIAL: quanto mais longe, pior fica\n",
    "            x_penalty = self.dist_penalty * (np.exp(abs(x)) - 1)\n",
    "            shaping -= x_penalty\n",
    "            \n",
    "            # PENALIZAR: Altura (y > 0 = acima do solo)\n",
    "            # No LunarLander, helipad está aproximadamente em y=0\n",
    "            # Penalização EXPONENCIAL: quanto mais alto, muito pior\n",
    "            height_above = max(0.0, y)\n",
    "            y_penalty = self.height_penalty * (np.exp(height_above) - 1) * 2\n",
    "            shaping -= y_penalty\n",
    "\n",
    "            # Penalalizar velocidade consoante mais perto do solo\n",
    "            \n",
    "            if y < 1.0:\n",
    "                vy = float(obs[3])\n",
    "                vel_penalty = abs(vy) * (2 - y)  # Quanto mais perto do solo, maior a penalização\n",
    "                shaping -= vel_penalty\n",
    "\n",
    "            if leg1_contact or leg2_contact:\n",
    "                vx = float(obs[2])\n",
    "                ground_vel_penalty = abs(vx) * 5.0  # Penalização maior por velocidade horizontal ao tocar o solo\n",
    "                shaping -= ground_vel_penalty\n",
    "                if leg1_contact and leg2_contact:\n",
    "                    shaping += 20.0  # Pequeno bónus por tocar com ambas as pernas\n",
    "\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[RewardShaping ERROR] {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            shaping = 0.0\n",
    "        \n",
    "        # Adicionar shaping ao reward original\n",
    "        shaped_reward = reward + shaping\n",
    "        \n",
    "        return obs, shaped_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4261db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindForceWrapper(gym.Wrapper):\n",
    "    \"\"\"Aplica perturbação lateral (vento) à velocidade horizontal 'observada'.\"\"\"\n",
    "    def __init__(self, env, wind_strength=0.02, deterministic=False):\n",
    "        super().__init__(env)\n",
    "        self.wind_strength = wind_strength\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done_flag = (terminated or truncated)\n",
    "\n",
    "        if isinstance(obs, np.ndarray) and obs.shape[0] >= 3:\n",
    "            wind = self.wind_strength if self.deterministic else self.wind_strength * (2*np.random.rand()-1)\n",
    "            obs = obs.copy()\n",
    "            obs[2] += wind\n",
    "            reward -= abs(wind) * 5.0\n",
    "\n",
    "        # devolver no mesmo formato do original\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f1c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObservationNoiseWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Add gaussian noise to observations to make the task more robust.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noise_std=0.01):\n",
    "        super().__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            return observation + np.random.normal(scale=self.noise_std, size=observation.shape)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b674",
   "metadata": {},
   "source": [
    "# Criação do ambiente customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d41948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_factory(env_id='LunarLander-v3', seed=None, config_name='orig', monitor_dir=None):\n",
    "    \"\"\"\n",
    "    Retorna uma função _init compatível com DummyVecEnv que:\n",
    "     - cria env,\n",
    "     - aplica wrappers consoante config_name,\n",
    "     - envolve com Monitor(escreve monitor.csv em monitor_dir).\n",
    "    config_name em {'orig', 'reward', 'wind', 'noise', 'all'}\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        if seed is not None:\n",
    "            # seed reset (Gymnasium)\n",
    "            try:\n",
    "                env.reset(seed=seed)\n",
    "            except TypeError:\n",
    "                env.reset()\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "\n",
    "        # Aplicar wrappers conforme config\n",
    "        if config_name == 'reward':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=2.0, land_bonus=100.0)\n",
    "        elif config_name == 'wind':\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "        elif config_name == 'noise':\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        elif config_name == 'all':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=2.0, land_bonus=80.0)\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        # caso 'orig' => nenhum wrapper\n",
    "\n",
    "        # Monitor: grava os episódios neste ficheiro\n",
    "        if monitor_dir is not None:\n",
    "            os.makedirs(monitor_dir, exist_ok=True)\n",
    "            monitor_file = os.path.join(monitor_dir, 'monitor.csv')\n",
    "            env = Monitor(env, filename=monitor_file)\n",
    "        else:\n",
    "            env = Monitor(env)\n",
    "\n",
    "        return env\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe23f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_factory_multiple_configs(env_id='LunarLander-v3', seed=None, config_name='orig', monitor_dir=None):\n",
    "    \"\"\"\n",
    "    Retorna uma função _init compatível com DummyVecEnv que:\n",
    "     - cria env,\n",
    "     - aplica wrappers consoante config_name,\n",
    "     - envolve com Monitor(escreve monitor.csv em monitor_dir).\n",
    "    config_name em {'orig', 'reward', 'wind', 'noise', 'all'}\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        if seed is not None:\n",
    "            # seed reset (Gymnasium)\n",
    "            try:\n",
    "                env.reset(seed=seed)\n",
    "            except TypeError:\n",
    "                env.reset()\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "\n",
    "        # Aplicar wrappers conforme config\n",
    "        if 'reward' in config_name:\n",
    "            print(\"[CREATING ENV] Applying RewardShapingWrapper\")\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=100.0)\n",
    "        if 'wind' in config_name:\n",
    "            print(\"[CREATING ENV] Applying WindForceWrapper\")\n",
    "            env = WindForceWrapper(env, wind_strength=0.01, deterministic=False)\n",
    "        if 'noise' in config_name:\n",
    "            print(\"[CREATING ENV] Applying ObservationNoiseWrapper\")\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        if config_name == 'all':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=80.0)\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        \n",
    "        # caso 'orig' => nenhum wrapper\n",
    "        \n",
    "        # Monitor: grava os episódios neste ficheiro\n",
    "        if monitor_dir is not None:\n",
    "            os.makedirs(monitor_dir, exist_ok=True)\n",
    "            monitor_file = os.path.join(monitor_dir, 'monitor.csv')\n",
    "            env = Monitor(env, filename=monitor_file)\n",
    "        else:\n",
    "            env = Monitor(env)\n",
    "\n",
    "        return env\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90f4e3",
   "metadata": {},
   "source": [
    "# Função de treino do PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_configs(config_name='orig', seed=0, timesteps=300_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Treina um PPO para a configuração especificada.\n",
    "    - Guarda modelo e monitor.csv em out_root/config_name/seedX/\n",
    "    - hyperparams: dict que sobrepõe os defaults do PPO (learning_rate,n_steps,batch_size,n_epochs,...)\n",
    "    \"\"\"\n",
    "    if hyperparams is None:\n",
    "        hyperparams = {}\n",
    "\n",
    "    name = ''\n",
    "\n",
    "    for i, c in enumerate(config_name):\n",
    "        name += c\n",
    "        if (i < len(config_name) - 1):\n",
    "            name += '_'\n",
    "\n",
    "    out_dir = os.path.join(out_root, name, f'seed{seed}')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # reproducibilidade\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    # criar env factory (monitor dentro da pasta out_dir)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=out_dir)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    # policy defaults\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[dict(pi=[256,256], vf=[256,256])])\n",
    "\n",
    "    # PPO defaults (podes sobrepor via hyperparams)\n",
    "    ppo_defaults = dict(\n",
    "        policy='MlpPolicy',\n",
    "        env=vec_env,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        learning_rate=3e-5,\n",
    "        n_steps=4096,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.0,\n",
    "        vf_coef=0.5,\n",
    "        clip_range=0.2,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        tensorboard_log=os.path.join(out_dir, 'tb')\n",
    "    )\n",
    "    # update defaults with provided hyperparams\n",
    "    ppo_defaults.update(hyperparams)\n",
    "\n",
    "    model = PPO(**ppo_defaults)\n",
    "\n",
    "    # callbacks: evaluation e checkpoints (guardam em out_dir)\n",
    "    eval_env = DummyVecEnv([make_env_factory_multiple_configs(seed=seed+1000, config_name=config_name, monitor_dir=None)])\n",
    "    eval_env = VecMonitor(eval_env)\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=out_dir,\n",
    "                                 log_path=out_dir, eval_freq=max(10_000, ppo_defaults['n_steps']*2),\n",
    "                                 n_eval_episodes=5, deterministic=True, render=False)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=max(50_000, ppo_defaults['n_steps']*5),\n",
    "                                             save_path=out_dir, name_prefix='ppo_checkpoint')\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "    model_path = os.path.join(out_dir, f'ppo_{config_name}_seed{seed}.zip')\n",
    "    model.save(model_path)\n",
    "\n",
    "    vec_env.close()\n",
    "    eval_env.close()\n",
    "    print(f\"[TRAIN] Saved model: {model_path}\")\n",
    "    return model_path, out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bf8b5",
   "metadata": {},
   "source": [
    "# Função de avaliação com critério revisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7120a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_custom(model_path, config_name='orig', seed=None, episodes=50):\n",
    "    \"\"\"\n",
    "    Avalia o modelo (usa DummyVecEnv com mesma config). Critério de sucesso:\n",
    "     - em qualquer step do episódio both legs touched, OR total_reward >= 200\n",
    "    Retorna dicionário com métricas e lista de recompensas por episódio.\n",
    "    \"\"\"\n",
    "    # carregar env (para avaliação, monitor não é necessário)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=None)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    model = PPO.load(model_path, env=vec_env)\n",
    "\n",
    "    # avaliação rápida via evaluate_policy (apenas para ter mean/std)\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=min(10, episodes), deterministic=True)\n",
    "    print(f\"[EVAL] quick evaluate_policy: mean={mean_reward:.2f} std={std_reward:.2f}\")\n",
    "\n",
    "    # per-episode sampling para success metric\n",
    "    successes = 0\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        reset_out = vec_env.reset()\n",
    "        obs = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "\n",
    "        done = False\n",
    "        total_r = 0.0\n",
    "        landed_flag = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            step_out = vec_env.step(action)\n",
    "            # VecEnv.step returns (obs, reward, done, info)\n",
    "            obs, reward, done, info = step_out\n",
    "            # reward pode ser array shape (1,), garantir float\n",
    "            try:\n",
    "                total_r += float(np.array(reward).item())\n",
    "            except Exception:\n",
    "                total_r += float(reward)\n",
    "\n",
    "            # verificar contacto das pernas na observação atual\n",
    "            try:\n",
    "                last_obs = obs[0]  # porque DummyVecEnv usa batch dimension\n",
    "                if bool(last_obs[6]) and bool(last_obs[7]):\n",
    "                    landed_flag = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if landed_flag or total_r >= 200:\n",
    "            successes += 1\n",
    "        rewards.append(total_r)\n",
    "\n",
    "    vec_env.close()\n",
    "    rewards = np.array(rewards)\n",
    "    result = {\n",
    "        'mean_reward': float(rewards.mean()),\n",
    "        'std_reward': float(rewards.std()),\n",
    "        'median_reward': float(np.median(rewards)),\n",
    "        'success_rate': float(successes) / len(rewards),\n",
    "        'per_episode': rewards\n",
    "    }\n",
    "    print(f\"[EVAL] result: mean={result['mean_reward']:.2f} std={result['std_reward']:.2f} success_rate={result['success_rate']:.2f}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795d9f7",
   "metadata": {},
   "source": [
    "# Função para plot de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069eafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_monitor(monitor_csv_path, window=10, show=True, out_png=None):\n",
    "    \"\"\"Plota rewards (raw + smoothed) a partir do monitor.csv gerado pelo Monitor.\"\"\"\n",
    "    if not os.path.exists(monitor_csv_path):\n",
    "        print(\"[PLOT] monitor file not found:\", monitor_csv_path)\n",
    "        return\n",
    "    df = pd.read_csv(monitor_csv_path, comment='#')\n",
    "    df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(df['r'], alpha=0.25, label='raw')\n",
    "    plt.plot(df['r_smooth'], label=f'smoothed({window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(os.path.basename(os.path.dirname(monitor_csv_path)))\n",
    "    plt.legend()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, bbox_inches='tight')\n",
    "        print(\"[PLOT] saved to\", out_png)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_compare_configs(base_dir, configs, seed, window=20):\n",
    "    \"\"\"Plota curvas suavizadas (por seed) comparando várias configs no mesmo gráfico.\"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for cfg in configs:\n",
    "        monitor_csv = os.path.join(base_dir, cfg, f'seed{seed}', 'monitor.csv')\n",
    "        if not os.path.exists(monitor_csv):\n",
    "            print(\"[COMPARE] monitor not found for\", cfg, \"seed\", seed)\n",
    "            continue\n",
    "        df = pd.read_csv(monitor_csv, comment='#')\n",
    "        df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "        plt.plot(df['r_smooth'], label=f'{cfg}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Smoothed Reward')\n",
    "    plt.title(f'Compare configs (seed {seed})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c532d",
   "metadata": {},
   "source": [
    "# Execução para múltiplas seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6687cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Roda treinos e avaliações para lista de configs e seeds.\n",
    "    Retorna um dict results[config][seed] = metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for s in seeds:\n",
    "        print(\"\\n\\n============================\")\n",
    "        print(f\"Training config={configs} seed={s}\")\n",
    "        model_path, out_dir = train_configs(config_name=configs, seed=s, timesteps=timesteps,\n",
    "                                            hyperparams=hyperparams, out_root=out_root)\n",
    "        print(f\"Evaluating model for config={configs} seed={s}\")\n",
    "        res = evaluate_custom(model_path=model_path, config_name=configs, seed=s, episodes=50)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37fd07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def run_hyperparam_grid(config_name, seed, grid, timesteps=300_000, out_root='./experiments_grid'):\n",
    "    \"\"\"\n",
    "    grid: dict of lists, e.g. {'learning_rate':[3e-5,1e-4],'n_steps':[2048,4096]}\n",
    "    Vai gerar todas as combinações, treinar e guardar resultados em out_root/config_name/seed/hparam_i\n",
    "    Retorna lista de (hparam_dict, metrics)\n",
    "    \"\"\"\n",
    "    keys, values = zip(*grid.items())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    results = []\n",
    "    for i, combo in enumerate(combos):\n",
    "        print(f\"\\n---- Grid {i+1}/{len(combos)}: {combo}\")\n",
    "        # colocar cada combo numa subpasta\n",
    "        out_root_combo = os.path.join(out_root, config_name, f'seed{seed}', f'grid_{i}')\n",
    "        os.makedirs(out_root_combo, exist_ok=True)\n",
    "        model_path, _ = train_configs(config_name=config_name, seed=seed, timesteps=timesteps,\n",
    "                                    hyperparams=combo, out_root=out_root_combo)\n",
    "        metrics = evaluate_custom(model_path=model_path, config_name=config_name, seed=seed, episodes=30)\n",
    "        results.append((combo, metrics, model_path, out_root_combo))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a76166",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22926e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model_path, config_name='orig', episodes=5, seed=None, render_mode='human'):\n",
    "    \"\"\"\n",
    "    Carrega um modelo treinado e renderiza episódios para visualização.\n",
    "    configs = ['orig','reward','wind','noise','all']\n",
    "    seeds = [0, 7, 42]\n",
    "    results = run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments')\n",
    "    Args:\n",
    "        model_path: caminho para o ficheiro .zip do modelo\n",
    "        config_name: configuração do ambiente ('orig', 'reward', 'wind', 'noise', 'all')\n",
    "        episodes: número de episódios para visualizar\n",
    "        seed: seed para reprodutibilidade\n",
    "        render_mode: 'human' para janela ou 'rgb_array' para gravar\n",
    "    \"\"\"\n",
    "    # Criar ambiente COM renderização\n",
    "    env = gym.make('LunarLander-v3', render_mode=render_mode)\n",
    "    \n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    \n",
    "    # Aplicar mesmos wrappers usados no treino\n",
    "    if 'reward' in config_name:\n",
    "        print(\"[VISUALIZE] Applying RewardShapingWrapper\")\n",
    "        env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=2.0, land_bonus=100.0)\n",
    "    if 'wind' in config_name:\n",
    "        print(\"[VISUALIZE] Applying WindForceWrapper\")\n",
    "        env = WindForceWrapper(env, wind_strength=0.01, deterministic=False)\n",
    "    if 'noise' in config_name:\n",
    "        print(\"[VISUALIZE] Applying ObservationNoiseWrapper\")\n",
    "        env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "    if config_name == 'all':\n",
    "        env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=2.0, land_bonus=80.0)\n",
    "        env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "        env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Executar episódios\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"\\n=== Episódio {ep+1}/{episodes} ===\")\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if render_mode == 'human':\n",
    "                env.render()\n",
    "                time.sleep(0.01)  # pequeno delay para suavizar visualização\n",
    "                if done:\n",
    "                    time.sleep(1.0)  # pausa no final do episódio\n",
    "        \n",
    "\n",
    "        \n",
    "        print(f\"Recompensa total: {total_reward:.2f} | Steps: {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nVisualização concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eea6c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================\n",
      "Training config=['reward', 'wind'] seed=73\n",
      "Using cuda device\n",
      "[CREATING ENV] Applying RewardShapingWrapper\n",
      "[RewardShapingWrapper] Exponential penalties: height_penalty=1.0, dist_penalty=0.8\n",
      "[CREATING ENV] Applying WindForceWrapper\n",
      "Logging to ./experiments/rewardwind/seed73/tb/PPO_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filipe/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n",
      "/home/filipe/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "/home/filipe/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.7     |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 1179     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "[TRAIN] Saved model: ./experiments/rewardwind/seed73/ppo_['reward', 'wind']_seed73.zip\n",
      "Evaluating model for config=['reward', 'wind'] seed=73\n",
      "[EVAL] quick evaluate_policy: mean=-783.89 std=357.17\n",
      "[EVAL] result: mean=-862.39 std=527.51 success_rate=0.00\n"
     ]
    }
   ],
   "source": [
    "configs = ['reward', 'wind']\n",
    "seeds = [random.randint(0, 100)]\n",
    "results = run_experiments(configs, seeds, timesteps=1_000, hyperparams=None, out_root='./experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eaa067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VISUALIZE] Applying RewardShapingWrapper\n",
      "[RewardShapingWrapper] Exponential penalties: height_penalty=2.0, dist_penalty=0.8\n",
      "[VISUALIZE] Applying WindForceWrapper\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"experiments/['reward', 'wind']/seed73/ppo_['reward', 'wind']_seed73.zip.zip\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualizar o melhor modelo da config 'all', seed 0\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mvisualize_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./experiments/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfigs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/seed\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/ppo_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfigs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_seed\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.zip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mvisualize_model\u001b[39m\u001b[34m(model_path, config_name, episodes, seed, render_mode)\u001b[39m\n\u001b[32m     33\u001b[39m     env = ObservationNoiseWrapper(env, noise_std=\u001b[32m0.02\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Carregar modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m model = \u001b[43mPPO\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Executar episódios\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/base_class.py:681\u001b[39m, in \u001b[36mBaseAlgorithm.load\u001b[39m\u001b[34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m     get_system_info()\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m data, params, pytorch_variables = \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo data found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo params found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/save_util.py:403\u001b[39m, in \u001b[36mload_from_zip_file\u001b[39m\u001b[34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_zip_file\u001b[39m(\n\u001b[32m    377\u001b[39m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path, io.BufferedIOBase],\n\u001b[32m    378\u001b[39m     load_data: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     print_system_info: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[32m    386\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     file = \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[32m    406\u001b[39m     device = get_device(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/functools.py:934\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    933\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/save_util.py:240\u001b[39m, in \u001b[36mopen_path_str\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;129m@open_path\u001b[39m.register(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> io.BufferedIOBase:\n\u001b[32m    227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    that the path exists.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m \u001b[33;03m    :return:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/save_util.py:291\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    285\u001b[39m         path.parent.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/save_util.py:272\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    270\u001b[39m             path, suffix = newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/site-packages/stable_baselines3/common/save_util.py:264\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/isia2/lib/python3.13/pathlib/_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: \"experiments/['reward', 'wind']/seed73/ppo_['reward', 'wind']_seed73.zip.zip\""
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualizar o melhor modelo da config 'all', seed 0\n",
    "visualize_model(\n",
    "    model_path=f'./experiments/{configs}/seed{seeds[0]}/ppo_{configs}_seed{seeds[0]}.zip',\n",
    "    config_name=configs,\n",
    "    episodes=1,\n",
    "    seed=seeds[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548630a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isia2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
