{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ee9226",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4cf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gymnasium e wrappers\n",
    "import gymnasium as gym\n",
    "\n",
    "# Stable-Baselines3\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f69d3",
   "metadata": {},
   "source": [
    "# Wrappers para customizações do LunarLander-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22601665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShapingWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Reward shaping baseado nas observações do LunarLander.\n",
    "    Obs: [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\n",
    "    \"\"\"\n",
    "    def __init__(self, env, angle_penalty=20.0, dist_penalty=0.5, height_penalty=1.0, land_bonus=100.0):\n",
    "        super().__init__(env)\n",
    "        self.angle_penalty = angle_penalty\n",
    "        self.dist_penalty = dist_penalty\n",
    "        self.land_bonus = land_bonus\n",
    "        self.height_penalty = height_penalty\n",
    "        print(f\"[RewardShapingWrapper] Exponential penalties: height_penalty={height_penalty}, dist_penalty={dist_penalty}\")\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Calcular reward shaping baseado na observação\n",
    "        shaping = 0.0\n",
    "        \n",
    "        try:\n",
    "            # LunarLander obs: [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\n",
    "            x = float(obs[0])\n",
    "            y = float(obs[1])\n",
    "            angle = float(obs[4])\n",
    "            angular_vel = float(obs[5])\n",
    "            leg1_contact = bool(obs[6])\n",
    "            leg2_contact = bool(obs[7])\n",
    "            \n",
    "            # BÔNUS: Aterrar com ambas as pernas\n",
    "            if leg1_contact and leg2_contact:\n",
    "                shaping += self.land_bonus\n",
    "\n",
    "            # PENALIZAR: Ângulo e velocidade angular (queremos horizontal e estável)\n",
    "            shaping -= self.angle_penalty * (abs(angle) + abs(angular_vel))\n",
    "\n",
    "            # PENALIZAR: Distância horizontal ao centro (x=0 é o helipad)\n",
    "            # Penalização EXPONENCIAL: quanto mais longe, pior fica\n",
    "            x_penalty = self.dist_penalty * (np.exp(abs(x)) - 1)\n",
    "            shaping -= x_penalty\n",
    "            \n",
    "            # PENALIZAR: Altura (y > 0 = acima do solo)\n",
    "            # No LunarLander, helipad está aproximadamente em y=0\n",
    "            # Penalização EXPONENCIAL: quanto mais alto, muito pior\n",
    "            height_above = max(0.0, y)\n",
    "            y_penalty = self.height_penalty * (np.exp(height_above) - 1)\n",
    "            shaping -= y_penalty\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[RewardShaping ERROR] {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            shaping = 0.0\n",
    "        \n",
    "        # Adicionar shaping ao reward original\n",
    "        shaped_reward = reward + shaping\n",
    "        \n",
    "        return obs, shaped_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4261db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindForceWrapper(gym.Wrapper):\n",
    "    \"\"\"Aplica perturbação lateral (vento) à velocidade horizontal 'observada'.\"\"\"\n",
    "    def __init__(self, env, wind_strength=0.02, deterministic=False):\n",
    "        super().__init__(env)\n",
    "        self.wind_strength = wind_strength\n",
    "        self.deterministic = deterministic\n",
    "\n",
    "    def step(self, action):\n",
    "        # Mantemos a assinatura Gymnasium: (obs, reward, terminated, truncated, info)\n",
    "        out = self.env.step(action)\n",
    "        # Pode ser 5-tuple (gymnasium) ou 4-tuple dependendo do wrapper; tratamos ambos\n",
    "        if len(out) == 5:\n",
    "            obs, reward, terminated, truncated, info = out\n",
    "            done_flag = (terminated or truncated)\n",
    "        else:\n",
    "            obs, reward, done_flag, info = out\n",
    "            terminated = done_flag; truncated = False\n",
    "\n",
    "        if isinstance(obs, np.ndarray) and obs.shape[0] >= 3:\n",
    "            wind = self.wind_strength if self.deterministic else self.wind_strength * (2*np.random.rand()-1)\n",
    "            obs = obs.copy()\n",
    "            obs[2] += wind\n",
    "            reward -= abs(wind) * 5.0\n",
    "\n",
    "        # devolver no mesmo formato do original\n",
    "        if len(out) == 5:\n",
    "            return obs, reward, terminated, truncated, info\n",
    "        else:\n",
    "            return obs, reward, done_flag, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f1c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ObservationNoiseWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Add gaussian noise to observations to make the task more robust.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noise_std=0.01):\n",
    "        super().__init__(env)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if isinstance(observation, np.ndarray):\n",
    "            return observation + np.random.normal(scale=self.noise_std, size=observation.shape)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b674",
   "metadata": {},
   "source": [
    "# Criação do ambiente customizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d41948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_factory(env_id='LunarLander-v3', seed=None, config_name='orig', monitor_dir=None):\n",
    "    \"\"\"\n",
    "    Retorna uma função _init compatível com DummyVecEnv que:\n",
    "     - cria env,\n",
    "     - aplica wrappers consoante config_name,\n",
    "     - envolve com Monitor(escreve monitor.csv em monitor_dir).\n",
    "    config_name em {'orig', 'reward', 'wind', 'noise', 'all'}\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        if seed is not None:\n",
    "            # seed reset (Gymnasium)\n",
    "            try:\n",
    "                env.reset(seed=seed)\n",
    "            except TypeError:\n",
    "                env.reset()\n",
    "            env.action_space.seed(seed)\n",
    "            env.observation_space.seed(seed)\n",
    "\n",
    "        # Aplicar wrappers conforme config\n",
    "        if config_name == 'reward':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=100.0)\n",
    "        elif config_name == 'wind':\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "        elif config_name == 'noise':\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        elif config_name == 'all':\n",
    "            env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=80.0)\n",
    "            env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "            env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "        # caso 'orig' => nenhum wrapper\n",
    "\n",
    "        # Monitor: grava os episódios neste ficheiro\n",
    "        if monitor_dir is not None:\n",
    "            os.makedirs(monitor_dir, exist_ok=True)\n",
    "            monitor_file = os.path.join(monitor_dir, 'monitor.csv')\n",
    "            env = Monitor(env, filename=monitor_file)\n",
    "        else:\n",
    "            env = Monitor(env)\n",
    "\n",
    "        return env\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90f4e3",
   "metadata": {},
   "source": [
    "# Função de treino do PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6054db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_config(config_name='orig', seed=0, timesteps=300_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Treina um PPO para a configuração especificada.\n",
    "    - Guarda modelo e monitor.csv em out_root/config_name/seedX/\n",
    "    - hyperparams: dict que sobrepõe os defaults do PPO (learning_rate,n_steps,batch_size,n_epochs,...)\n",
    "    \"\"\"\n",
    "    if hyperparams is None:\n",
    "        hyperparams = {}\n",
    "\n",
    "    out_dir = os.path.join(out_root, config_name, f'seed{seed}')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # reproducibilidade\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    # criar env factory (monitor dentro da pasta out_dir)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=out_dir)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    # policy defaults\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.Tanh, net_arch=[dict(pi=[256,256], vf=[256,256])])\n",
    "\n",
    "    # PPO defaults (podes sobrepor via hyperparams)\n",
    "    ppo_defaults = dict(\n",
    "        policy='MlpPolicy',\n",
    "        env=vec_env,\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        learning_rate=3e-5,\n",
    "        n_steps=4096,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.0,\n",
    "        vf_coef=0.5,\n",
    "        clip_range=0.2,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        tensorboard_log=os.path.join(out_dir, 'tb')\n",
    "    )\n",
    "    # update defaults with provided hyperparams\n",
    "    ppo_defaults.update(hyperparams)\n",
    "\n",
    "    model = PPO(**ppo_defaults)\n",
    "\n",
    "    # callbacks: evaluation e checkpoints (guardam em out_dir)\n",
    "    eval_env = DummyVecEnv([make_env_factory(seed=seed+1000, config_name=config_name, monitor_dir=None)])\n",
    "    eval_env = VecMonitor(eval_env)\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path=out_dir,\n",
    "                                 log_path=out_dir, eval_freq=max(10_000, ppo_defaults['n_steps']*2),\n",
    "                                 n_eval_episodes=5, deterministic=True, render=False)\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=max(50_000, ppo_defaults['n_steps']*5),\n",
    "                                             save_path=out_dir, name_prefix='ppo_checkpoint')\n",
    "\n",
    "    model.learn(total_timesteps=timesteps, callback=[eval_callback, checkpoint_callback])\n",
    "\n",
    "    model_path = os.path.join(out_dir, f'ppo_{config_name}_seed{seed}.zip')\n",
    "    model.save(model_path)\n",
    "\n",
    "    vec_env.close()\n",
    "    eval_env.close()\n",
    "    print(f\"[TRAIN] Saved model: {model_path}\")\n",
    "    return model_path, out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bf8b5",
   "metadata": {},
   "source": [
    "# Função de avaliação com critério revisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7120a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_custom(model_path, config_name='orig', seed=None, episodes=50):\n",
    "    \"\"\"\n",
    "    Avalia o modelo (usa DummyVecEnv com mesma config). Critério de sucesso:\n",
    "     - em qualquer step do episódio both legs touched, OR total_reward >= 200\n",
    "    Retorna dicionário com métricas e lista de recompensas por episódio.\n",
    "    \"\"\"\n",
    "    # carregar env (para avaliação, monitor não é necessário)\n",
    "    env_fn = make_env_factory(seed=seed, config_name=config_name, monitor_dir=None)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "    vec_env = VecMonitor(vec_env)\n",
    "\n",
    "    model = PPO.load(model_path, env=vec_env)\n",
    "\n",
    "    # avaliação rápida via evaluate_policy (apenas para ter mean/std)\n",
    "    mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=min(10, episodes), deterministic=True)\n",
    "    print(f\"[EVAL] quick evaluate_policy: mean={mean_reward:.2f} std={std_reward:.2f}\")\n",
    "\n",
    "    # per-episode sampling para success metric\n",
    "    successes = 0\n",
    "    rewards = []\n",
    "    for _ in range(episodes):\n",
    "        reset_out = vec_env.reset()\n",
    "        obs = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
    "\n",
    "        done = False\n",
    "        total_r = 0.0\n",
    "        landed_flag = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            step_out = vec_env.step(action)\n",
    "            # VecEnv.step returns (obs, reward, done, info)\n",
    "            obs, reward, done, info = step_out\n",
    "            # reward pode ser array shape (1,), garantir float\n",
    "            try:\n",
    "                total_r += float(np.array(reward).item())\n",
    "            except Exception:\n",
    "                total_r += float(reward)\n",
    "\n",
    "            # verificar contacto das pernas na observação atual\n",
    "            try:\n",
    "                last_obs = obs[0]  # porque DummyVecEnv usa batch dimension\n",
    "                if bool(last_obs[6]) and bool(last_obs[7]):\n",
    "                    landed_flag = True\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if landed_flag or total_r >= 200:\n",
    "            successes += 1\n",
    "        rewards.append(total_r)\n",
    "\n",
    "    vec_env.close()\n",
    "    rewards = np.array(rewards)\n",
    "    result = {\n",
    "        'mean_reward': float(rewards.mean()),\n",
    "        'std_reward': float(rewards.std()),\n",
    "        'median_reward': float(np.median(rewards)),\n",
    "        'success_rate': float(successes) / len(rewards),\n",
    "        'per_episode': rewards\n",
    "    }\n",
    "    print(f\"[EVAL] result: mean={result['mean_reward']:.2f} std={result['std_reward']:.2f} success_rate={result['success_rate']:.2f}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795d9f7",
   "metadata": {},
   "source": [
    "# Função para plot de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069eafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_monitor(monitor_csv_path, window=10, show=True, out_png=None):\n",
    "    \"\"\"Plota rewards (raw + smoothed) a partir do monitor.csv gerado pelo Monitor.\"\"\"\n",
    "    if not os.path.exists(monitor_csv_path):\n",
    "        print(\"[PLOT] monitor file not found:\", monitor_csv_path)\n",
    "        return\n",
    "    df = pd.read_csv(monitor_csv_path, comment='#')\n",
    "    df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(df['r'], alpha=0.25, label='raw')\n",
    "    plt.plot(df['r_smooth'], label=f'smoothed({window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(os.path.basename(os.path.dirname(monitor_csv_path)))\n",
    "    plt.legend()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, bbox_inches='tight')\n",
    "        print(\"[PLOT] saved to\", out_png)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_compare_configs(base_dir, configs, seed, window=20):\n",
    "    \"\"\"Plota curvas suavizadas (por seed) comparando várias configs no mesmo gráfico.\"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for cfg in configs:\n",
    "        monitor_csv = os.path.join(base_dir, cfg, f'seed{seed}', 'monitor.csv')\n",
    "        if not os.path.exists(monitor_csv):\n",
    "            print(\"[COMPARE] monitor not found for\", cfg, \"seed\", seed)\n",
    "            continue\n",
    "        df = pd.read_csv(monitor_csv, comment='#')\n",
    "        df['r_smooth'] = df['r'].rolling(window=window, min_periods=1).mean()\n",
    "        plt.plot(df['r_smooth'], label=f'{cfg}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Smoothed Reward')\n",
    "    plt.title(f'Compare configs (seed {seed})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c532d",
   "metadata": {},
   "source": [
    "# Execução para múltiplas seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef02740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments'):\n",
    "    \"\"\"\n",
    "    Roda treinos e avaliações para lista de configs e seeds.\n",
    "    Retorna um dict results[config][seed] = metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for cfg in configs:\n",
    "        results[cfg] = {}\n",
    "        for s in seeds:\n",
    "            print(\"\\n\\n============================\")\n",
    "            print(f\"Training config={cfg} seed={s}\")\n",
    "            model_path, out_dir = train_config(config_name=cfg, seed=s, timesteps=timesteps,\n",
    "                                               hyperparams=hyperparams, out_root=out_root)\n",
    "            print(f\"Evaluating model for config={cfg} seed={s}\")\n",
    "            res = evaluate_custom(model_path=model_path, config_name=cfg, seed=s, episodes=50)\n",
    "            results[cfg][s] = res\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fd07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def run_hyperparam_grid(config_name, seed, grid, timesteps=300_000, out_root='./experiments_grid'):\n",
    "    \"\"\"\n",
    "    grid: dict of lists, e.g. {'learning_rate':[3e-5,1e-4],'n_steps':[2048,4096]}\n",
    "    Vai gerar todas as combinações, treinar e guardar resultados em out_root/config_name/seed/hparam_i\n",
    "    Retorna lista de (hparam_dict, metrics)\n",
    "    \"\"\"\n",
    "    keys, values = zip(*grid.items())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    results = []\n",
    "    for i, combo in enumerate(combos):\n",
    "        print(f\"\\n---- Grid {i+1}/{len(combos)}: {combo}\")\n",
    "        # colocar cada combo numa subpasta\n",
    "        out_root_combo = os.path.join(out_root, config_name, f'seed{seed}', f'grid_{i}')\n",
    "        os.makedirs(out_root_combo, exist_ok=True)\n",
    "        model_path, _ = train_config(config_name=config_name, seed=seed, timesteps=timesteps,\n",
    "                                    hyperparams=combo, out_root=out_root_combo)\n",
    "        metrics = evaluate_custom(model_path=model_path, config_name=config_name, seed=seed, episodes=30)\n",
    "        results.append((combo, metrics, model_path, out_root_combo))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a76166",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22926e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model_path, config_name='orig', episodes=5, seed=None, render_mode='human'):\n",
    "    \"\"\"\n",
    "    Carrega um modelo treinado e renderiza episódios para visualização.\n",
    "    configs = ['orig','reward','wind','noise','all']\n",
    "    seeds = [0, 7, 42]\n",
    "    results = run_experiments(configs, seeds, timesteps=50_000, hyperparams=None, out_root='./experiments')\n",
    "    Args:\n",
    "        model_path: caminho para o ficheiro .zip do modelo\n",
    "        config_name: configuração do ambiente ('orig', 'reward', 'wind', 'noise', 'all')\n",
    "        episodes: número de episódios para visualizar\n",
    "        seed: seed para reprodutibilidade\n",
    "        render_mode: 'human' para janela ou 'rgb_array' para gravar\n",
    "    \"\"\"\n",
    "    # Criar ambiente COM renderização\n",
    "    env = gym.make('LunarLander-v3', render_mode=render_mode)\n",
    "    \n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    \n",
    "    # Aplicar mesmos wrappers usados no treino\n",
    "    if config_name == 'reward':\n",
    "        env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=100.0)\n",
    "    elif config_name == 'wind':\n",
    "        env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "    elif config_name == 'noise':\n",
    "        env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "    elif config_name == 'all':\n",
    "        env = RewardShapingWrapper(env, angle_penalty=30.0, dist_penalty=0.8, height_penalty=1.0, land_bonus=80.0)\n",
    "        env = WindForceWrapper(env, wind_strength=0.03, deterministic=False)\n",
    "        env = ObservationNoiseWrapper(env, noise_std=0.02)\n",
    "    \n",
    "    # Carregar modelo\n",
    "    model = PPO.load(model_path)\n",
    "    \n",
    "    # Executar episódios\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"\\n=== Episódio {ep+1}/{episodes} ===\")\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if render_mode == 'human':\n",
    "                env.render()\n",
    "                time.sleep(0.01)  # pequeno delay para suavizar visualização\n",
    "        \n",
    "        print(f\"Recompensa total: {total_reward:.2f} | Steps: {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nVisualização concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6c5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" configs = ['reward']\\nseeds = [random.randint(0, 100)]\\nresults = run_experiments(configs, seeds, timesteps=500_000, hyperparams=None, out_root='./experiments') \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = ['reward']\n",
    "seeds = [random.randint(0, 100)]\n",
    "results = run_experiments(configs, seeds, timesteps=500_000, hyperparams=None, out_root='./experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50eaa067",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualizar o melhor modelo da config 'all', seed 0\u001b[39;00m\n\u001b[32m      2\u001b[39m visualize_model(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model_path=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./experiments/reward/seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mseeds\u001b[49m[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/ppo_reward_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseeds[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.zip\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     config_name=\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     episodes=\u001b[32m1\u001b[39m,\n\u001b[32m      6\u001b[39m     seed=seeds[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'seeds' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualizar o melhor modelo da config 'all', seed 0\n",
    "visualize_model(\n",
    "    model_path=f'./experiments/reward/seed{seeds[0]}/ppo_reward_seed{seeds[0]}.zip',\n",
    "    config_name='reward',\n",
    "    episodes=1,\n",
    "    seed=seeds[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac28e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o melhor modelo da config 'all', seed 0\n",
    "seeds = {97}\n",
    "visualize_model(\n",
    "    model_path=f'./experiments/reward/seed{seeds[0]}/ppo_reward_seed{seeds[0]}.zip',\n",
    "    config_name='reward',\n",
    "    episodes=1,\n",
    "    seed=seeds[0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isia2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
